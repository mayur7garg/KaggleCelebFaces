{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Convolutional Generative Adversarial Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import time\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.train import Checkpoint, CheckpointManager\r\n",
    "from tensorflow.data import Dataset\r\n",
    "from tensorflow.data.experimental import AUTOTUNE\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
    "from tensorflow.keras.metrics import Mean\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.initializers import TruncatedNormal, RandomNormal\r\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, BatchNormalization, Conv2D, Conv2DTranspose, \\\r\n",
    "        LeakyReLU, Flatten, SpatialDropout2D, Dropout, MaxPool2D, GlobalAvgPool2D, Concatenate, LayerNormalization\r\n",
    "\r\n",
    "from IPython import display\r\n",
    "\r\n",
    "print(tf.__version__)"
   ],
   "outputs": [],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-20T18:07:55.840300Z",
     "iopub.status.busy": "2021-07-20T18:07:55.831667Z",
     "iopub.status.idle": "2021-07-20T18:08:00.271614Z",
     "shell.execute_reply": "2021-07-20T18:08:00.271057Z",
     "shell.execute_reply.started": "2021-07-20T17:00:13.052743Z"
    },
    "papermill": {
     "duration": 4.459597,
     "end_time": "2021-07-20T18:08:00.271757",
     "exception": false,
     "start_time": "2021-07-20T18:07:55.812160",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Constants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BASE_PATH = r'../Dataset/img_align_celeba'\r\n",
    "RANDOM_STATE = 7\r\n",
    "SHUFFLE_BUFFER = 32_000\r\n",
    "IMAGE_SIZE = (192, 160)\r\n",
    "BATCH_SIZE = 32\r\n",
    "GEN_NOISE_SHAPE = (6, 5, 8)\r\n",
    "PREDICT_COUNT = 9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Constants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "GEN_LR = 1e-6\r\n",
    "GEN_BETA_1 = 0.5\r\n",
    "DISC_LR = 1e-6\r\n",
    "DISC_BETA_1 = 0.9\r\n",
    "GEN_RELU_ALPHA = 0.2\r\n",
    "DISC_RELU_ALPHA = 0.3\r\n",
    "EPOCHS = 20\r\n",
    "DISC_LABEL_SMOOTHING = 0.3\r\n",
    "PLOTS_DPI = 150\r\n",
    "RETRAIN = True"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:08:00.303058Z",
     "iopub.status.busy": "2021-07-20T18:08:00.302534Z",
     "iopub.status.idle": "2021-07-20T18:08:00.306023Z",
     "shell.execute_reply": "2021-07-20T18:08:00.306418Z",
     "shell.execute_reply.started": "2021-07-20T17:00:14.699078Z"
    },
    "papermill": {
     "duration": 0.022466,
     "end_time": "2021-07-20T18:08:00.306629",
     "exception": false,
     "start_time": "2021-07-20T18:08:00.284163",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading all image file names"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "image_names = Dataset.list_files(os.path.join(BASE_PATH, '*.jpg'), seed = RANDOM_STATE)\r\n",
    "image_count = image_names.cardinality().numpy()\r\n",
    "print(f\"\\nTotal number of image files: {image_count}\\n\")"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:08:01.991685Z",
     "iopub.status.busy": "2021-07-20T18:08:01.990955Z",
     "iopub.status.idle": "2021-07-20T18:10:25.466281Z",
     "shell.execute_reply": "2021-07-20T18:10:25.466711Z",
     "shell.execute_reply.started": "2021-07-20T17:00:14.707005Z"
    },
    "papermill": {
     "duration": 145.148578,
     "end_time": "2021-07-20T18:10:25.466869",
     "exception": false,
     "start_time": "2021-07-20T18:08:00.318291",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Data loading pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_image_data(filename):\r\n",
    "    img = tf.io.read_file(filename)\r\n",
    "    img = tf.io.decode_jpeg(img, channels = 3)\r\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\r\n",
    "    return (img - 127.5)/127.5\r\n",
    "\r\n",
    "train_ds = image_names.cache() \\\r\n",
    "        .shuffle(SHUFFLE_BUFFER, seed = RANDOM_STATE) \\\r\n",
    "        .map(load_image_data, num_parallel_calls = AUTOTUNE) \\\r\n",
    "        .batch(BATCH_SIZE, drop_remainder = True) \\\r\n",
    "        .prefetch(buffer_size = AUTOTUNE)\r\n",
    "\r\n",
    "train_ds"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:25.496637Z",
     "iopub.status.busy": "2021-07-20T18:10:25.496045Z",
     "iopub.status.idle": "2021-07-20T18:10:25.585180Z",
     "shell.execute_reply": "2021-07-20T18:10:25.585614Z",
     "shell.execute_reply.started": "2021-07-20T17:01:33.693890Z"
    },
    "papermill": {
     "duration": 0.106544,
     "end_time": "2021-07-20T18:10:25.585751",
     "exception": false,
     "start_time": "2021-07-20T18:10:25.479207",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image data samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (9, 11))\r\n",
    "\r\n",
    "sample_images = [i for i in train_ds.take(1)][0].numpy()\r\n",
    "\r\n",
    "for i, ax in enumerate(axes.flatten()):\r\n",
    "    ax.imshow((sample_images[i] * 0.5) + 0.5)\r\n",
    "    ax.axis(False)\r\n",
    "    ax.grid(False)\r\n",
    "\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:25.624239Z",
     "iopub.status.busy": "2021-07-20T18:10:25.623693Z",
     "iopub.status.idle": "2021-07-20T18:10:27.754197Z",
     "shell.execute_reply": "2021-07-20T18:10:27.754677Z",
     "shell.execute_reply.started": "2021-07-20T17:01:33.773199Z"
    },
    "papermill": {
     "duration": 2.155853,
     "end_time": "2021-07-20T18:10:27.754841",
     "exception": false,
     "start_time": "2021-07-20T18:10:25.598988",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Noise data input for `generator` model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seed_noise = tf.random.normal([PREDICT_COUNT, *GEN_NOISE_SHAPE])\r\n",
    "\r\n",
    "fig, axes = plt.subplots(nrows = PREDICT_COUNT, ncols = GEN_NOISE_SHAPE[2], figsize = (10, 14))\r\n",
    "\r\n",
    "for i in range(PREDICT_COUNT):\r\n",
    "    for j in range(GEN_NOISE_SHAPE[2]):\r\n",
    "        axes[i][j].imshow(seed_noise[i, :, :, j], cmap = 'gray')\r\n",
    "        axes[i][j].axis(False)\r\n",
    "        axes[i][j].grid(False)\r\n",
    "        axes[i][j].set_title(f\"Img {i + 1}, Ch {j + 1}\")\r\n",
    "        \r\n",
    "plt.suptitle(f'Input Seed data. Shape: {seed_noise.shape}', fontsize = 20)\r\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:27.818095Z",
     "iopub.status.busy": "2021-07-20T18:10:27.817531Z",
     "iopub.status.idle": "2021-07-20T18:10:31.223781Z",
     "shell.execute_reply": "2021-07-20T18:10:31.223332Z",
     "shell.execute_reply.started": "2021-07-20T17:01:35.141429Z"
    },
    "papermill": {
     "duration": 3.440605,
     "end_time": "2021-07-20T18:10:31.223898",
     "exception": false,
     "start_time": "2021-07-20T18:10:27.783293",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Creation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generator_model():\r\n",
    "    weight_init = TruncatedNormal(mean = 0.0, stddev = 0.02)\r\n",
    "\r\n",
    "    input_layer = Input(shape = GEN_NOISE_SHAPE, name = 'Gen_Input')\r\n",
    "    flatten = Flatten(name = 'Gen_Flatten')(input_layer)\r\n",
    "    dense = Dense(6 * 5 * 512, use_bias = False, kernel_initializer = weight_init, \r\n",
    "                activation = LeakyReLU(GEN_RELU_ALPHA), name = 'Gen_Dense')(flatten)\r\n",
    "    reshape = Reshape((6, 5, 512), name = 'Gen_Reshape')(dense)\r\n",
    "    sp_dropout_1 = SpatialDropout2D(0.1, name = 'Gen_SD_1')(reshape)\r\n",
    "\r\n",
    "\r\n",
    "    conv_T_1 = Conv2DTranspose(512, (3, 3), padding = 'same', activation = LeakyReLU(GEN_RELU_ALPHA), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_1')(sp_dropout_1) \r\n",
    "    conv_T_2 = Conv2DTranspose(256, (3, 3), padding = 'same', strides = (2, 2), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_2')(conv_T_1)\r\n",
    "    bn_1 = BatchNormalization(name = 'Gen_BN_1')(conv_T_2)\r\n",
    "    lr_1 = LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_1')(bn_1)   \r\n",
    "\r\n",
    "\r\n",
    "    conv_T_3 = Conv2DTranspose(128, (4, 4), padding = 'same', activation = LeakyReLU(GEN_RELU_ALPHA), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_3')(lr_1) \r\n",
    "    conv_T_4 = Conv2DTranspose(64, (4, 4), padding = 'same', strides = (2, 2), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_4')(conv_T_3)\r\n",
    "    bn_2 = BatchNormalization(name = 'Gen_BN_2')(conv_T_4)\r\n",
    "    lr_2 = LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_2')(bn_2)\r\n",
    "    sp_dropout_2 = SpatialDropout2D(0.1, name = 'Gen_SD_2')(lr_2)\r\n",
    "\r\n",
    "\r\n",
    "    conv_T_5 = Conv2DTranspose(16, (5, 5), padding = 'same', strides = (2, 2), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_5')(sp_dropout_2)\r\n",
    "    bn_3 = BatchNormalization(name = 'Gen_BN_3')(conv_T_5)\r\n",
    "    lr_3 = LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_3')(bn_3)   \r\n",
    "    sp_dropout_3 = SpatialDropout2D(0.1, name = 'Gen_SD_3')(lr_3)\r\n",
    "\r\n",
    "\r\n",
    "    conv_T_6 = Conv2DTranspose(8, (6, 6), padding = 'same', strides = (2, 2), use_bias = False, \r\n",
    "                               kernel_initializer = weight_init, name = 'Gen_Conv_T_6')(sp_dropout_3)\r\n",
    "    bn_4 = BatchNormalization(name = 'Gen_BN_4')(conv_T_6)\r\n",
    "    lr_4 = LeakyReLU(GEN_RELU_ALPHA, name = 'Gen_LR_4')(bn_4)\r\n",
    "    sp_dropout_4 = SpatialDropout2D(0.1, name = 'Gen_SD_4')(lr_4)\r\n",
    "\r\n",
    "\r\n",
    "    conv_T_7 = Conv2DTranspose(8, (7, 7), padding = 'same', activation = LeakyReLU(GEN_RELU_ALPHA), use_bias = False,\r\n",
    "                               kernel_initializer = weight_init, strides = (2, 2), name = 'Gen_Conv_T_7')(sp_dropout_4)\r\n",
    "    conv_T_8 = Conv2DTranspose(3, (5, 5), padding = 'same', kernel_initializer = weight_init, use_bias = False,\r\n",
    "                               activation = 'tanh', name = 'Gen_Conv_T_8')(conv_T_7)\r\n",
    "    \r\n",
    "    return Model(inputs = input_layer, outputs = conv_T_8, name = 'Generator')\r\n",
    "    \r\n",
    "generator = generator_model()\r\n",
    "generator.summary()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:31.296901Z",
     "iopub.status.busy": "2021-07-20T18:10:31.296271Z",
     "iopub.status.idle": "2021-07-20T18:10:31.453839Z",
     "shell.execute_reply": "2021-07-20T18:10:31.453347Z",
     "shell.execute_reply.started": "2021-07-20T17:01:38.601871Z"
    },
    "papermill": {
     "duration": 0.200324,
     "end_time": "2021-07-20T18:10:31.453964",
     "exception": false,
     "start_time": "2021-07-20T18:10:31.253640",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_model(generator, to_file = 'Generator.jpg', show_shapes = True, dpi = PLOTS_DPI)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discriminator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator_model():\r\n",
    "    input_layer = Input(shape = (*IMAGE_SIZE, 3), name = 'Disc_Input')\r\n",
    "    \r\n",
    "    conv_1 = Conv2D(32, (4, 4), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_1')(input_layer)\r\n",
    "    max_pool_1 = MaxPool2D(2, name = 'Disc_MP_1')(conv_1)\r\n",
    "    conv_2 = Conv2D(64, (4, 4), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_2')(max_pool_1)\r\n",
    "    max_pool_2 = MaxPool2D(2, name = 'Disc_MP_2')(conv_2)\r\n",
    "    global_pool_1 = GlobalAvgPool2D(name = 'Disc_GAP_1')(max_pool_2)\r\n",
    "    \r\n",
    "    sp_dropout_1 = SpatialDropout2D(0.2, name = 'Disc_SD_1')(max_pool_2)\r\n",
    "    conv_3 = Conv2D(128, (3, 3), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_3')(sp_dropout_1)\r\n",
    "    max_pool_3 = MaxPool2D(2, name = 'Disc_MP_3')(conv_3)\r\n",
    "    conv_4 = Conv2D(256, (3, 3), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_4')(max_pool_3)\r\n",
    "    max_pool_4 = MaxPool2D(2, name = 'Disc_MP_4')(conv_4)\r\n",
    "    global_pool_2 = GlobalAvgPool2D(name = 'Disc_GAP_2')(max_pool_4)\r\n",
    "    \r\n",
    "    sp_dropout_2 = SpatialDropout2D(0.2, name = 'Disc_SD_2')(max_pool_4)\r\n",
    "    conv_5 = Conv2D(512, (2, 2), activation = LeakyReLU(DISC_RELU_ALPHA), padding = 'same', name = 'Disc_Conv_5')(sp_dropout_2)\r\n",
    "    max_pool_5 = MaxPool2D(2, name = 'Disc_MP_5')(conv_5)\r\n",
    "    global_pool_3 = GlobalAvgPool2D(name = 'Disc_GAP_3')(max_pool_5)\r\n",
    "    \r\n",
    "    concat = Concatenate(name = 'Disc_Concat')([global_pool_1, global_pool_2, global_pool_3])\r\n",
    "    dropout = Dropout(0.2, name = 'Disc_Dropout')(concat)\r\n",
    "    dense_1 = Dense(32, activation = LeakyReLU(DISC_RELU_ALPHA), name = 'Disc_Dense_1')(dropout)\r\n",
    "    dense_2 = Dense(1, name = 'Disc_Dense_2')(dense_1)\r\n",
    "    \r\n",
    "    return Model(inputs = input_layer, outputs = dense_2, name = 'Discriminator')\r\n",
    "    \r\n",
    "discriminator = discriminator_model()\r\n",
    "discriminator.summary()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:38.602537Z",
     "iopub.status.busy": "2021-07-20T18:10:38.599826Z",
     "iopub.status.idle": "2021-07-20T18:10:38.699584Z",
     "shell.execute_reply": "2021-07-20T18:10:38.699166Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.097604Z"
    },
    "papermill": {
     "duration": 0.181312,
     "end_time": "2021-07-20T18:10:38.699704",
     "exception": false,
     "start_time": "2021-07-20T18:10:38.518392",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_model(discriminator, to_file = 'Discriminator.jpg', show_shapes = True, dpi = PLOTS_DPI)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss and Optimizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits = True)\r\n",
    "gen_mean_loss = Mean(name = \"Generator mean loss\")\r\n",
    "disc_mean_loss = Mean(name = \"Discriminator mean loss\")\r\n",
    "generator_optimizer = Adam(GEN_LR, beta_1 = GEN_BETA_1)\r\n",
    "discriminator_optimizer = Adam(DISC_LR, beta_1 = DISC_BETA_1)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:39.082914Z",
     "iopub.status.busy": "2021-07-20T18:10:39.082087Z",
     "iopub.status.idle": "2021-07-20T18:10:39.091341Z",
     "shell.execute_reply": "2021-07-20T18:10:39.091720Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.323996Z"
    },
    "papermill": {
     "duration": 0.087489,
     "end_time": "2021-07-20T18:10:39.091858",
     "exception": false,
     "start_time": "2021-07-20T18:10:39.004369",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restoring `checkpoint` via `CheckpointManager` if `RETRAIN` is `True`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint_dir = './ckpt'\r\n",
    "\r\n",
    "checkpoint = Checkpoint(\r\n",
    "    step = tf.Variable(1),\r\n",
    "    generator_optimizer = generator_optimizer,\r\n",
    "    discriminator_optimizer = discriminator_optimizer,\r\n",
    "    generator = generator,\r\n",
    "    discriminator = discriminator)\r\n",
    "\r\n",
    "ckpt_manager = CheckpointManager(checkpoint, checkpoint_dir, max_to_keep = 4)\r\n",
    "\r\n",
    "EPOCH_START = 1\r\n",
    "if RETRAIN:\r\n",
    "    checkpoint.restore(ckpt_manager.latest_checkpoint)\r\n",
    "    EPOCH_START = checkpoint.step.numpy()\r\n",
    "\r\n",
    "print(f\"Starting training from Epoch {EPOCH_START}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating images using the `generator` model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_images(seed, save = False, epoch = None):\r\n",
    "    pred = generator(seed, training = False)\r\n",
    "\r\n",
    "    fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (9, 11))\r\n",
    "\r\n",
    "    for i, ax in enumerate(axes.flatten()):\r\n",
    "        ax.imshow((pred[i] * 0.5) + 0.5)\r\n",
    "        ax.axis(False)\r\n",
    "        ax.grid(False)\r\n",
    "\r\n",
    "    plt.suptitle('Generator Predictions', fontsize = 20)\r\n",
    "    \r\n",
    "    plt.tight_layout()\r\n",
    "\r\n",
    "    if save:\r\n",
    "        plt.savefig(f'Outputs/Pred_Epoch_{epoch:04d}.png', dpi = PLOTS_DPI, facecolor = 'white', \r\n",
    "                transparent = False, bbox_inches = 'tight')\r\n",
    "        plt.close()\r\n",
    "    \r\n",
    "generate_images(seed_noise)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:31.524153Z",
     "iopub.status.busy": "2021-07-20T18:10:31.523547Z",
     "iopub.status.idle": "2021-07-20T18:10:38.451405Z",
     "shell.execute_reply": "2021-07-20T18:10:38.451828Z",
     "shell.execute_reply.started": "2021-07-20T17:01:38.770608Z"
    },
    "papermill": {
     "duration": 6.967375,
     "end_time": "2021-07-20T18:10:38.451983",
     "exception": false,
     "start_time": "2021-07-20T18:10:31.484608",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting on the generated images using the `discriminator` model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "discriminator(generator(seed_noise, training = False), training = False).numpy()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:38.837659Z",
     "iopub.status.busy": "2021-07-20T18:10:38.836912Z",
     "iopub.status.idle": "2021-07-20T18:10:38.938413Z",
     "shell.execute_reply": "2021-07-20T18:10:38.937934Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.238573Z"
    },
    "papermill": {
     "duration": 0.172785,
     "end_time": "2021-07-20T18:10:38.938556",
     "exception": false,
     "start_time": "2021-07-20T18:10:38.765771",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss functions for `generator` and `discriminator`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator_loss(real_output, fake_output):\r\n",
    "    pos_labels = tf.ones_like(real_output) - (tf.random.uniform(real_output.shape) * DISC_LABEL_SMOOTHING)\r\n",
    "    neg_labels = tf.zeros_like(fake_output) + (tf.random.uniform(fake_output.shape) * DISC_LABEL_SMOOTHING)\r\n",
    "    real_loss = cross_entropy(pos_labels, real_output)\r\n",
    "    fake_loss = cross_entropy(neg_labels, fake_output)\r\n",
    "    total_loss = real_loss + fake_loss\r\n",
    "    return total_loss\r\n",
    "\r\n",
    "def generator_loss(fake_output):\r\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:39.230445Z",
     "iopub.status.busy": "2021-07-20T18:10:39.229695Z",
     "iopub.status.idle": "2021-07-20T18:10:39.231947Z",
     "shell.execute_reply": "2021-07-20T18:10:39.232397Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.347712Z"
    },
    "papermill": {
     "duration": 0.074632,
     "end_time": "2021-07-20T18:10:39.232536",
     "exception": false,
     "start_time": "2021-07-20T18:10:39.157904",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training batch step\r\n",
    "1. Get input noise for the `generator`\r\n",
    "2. Generate images using the `generator` and the noise data\r\n",
    "3. Get predictions of the `discriminator` separately on the real as well as the generated images\r\n",
    "4. Calculate losses for the `generator` and the `discriminator` using the predictions of the previous step\r\n",
    "5. Update the `Mean` loss metric for the epoch\r\n",
    "6. Get gradients for the losses \r\n",
    "7. Apply the gradients to the `trainable_variables` of the `generator` and the `discriminator` using their optimizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\r\n",
    "def train_step(images):\r\n",
    "    noise = tf.random.normal([BATCH_SIZE, *GEN_NOISE_SHAPE])\r\n",
    "\r\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
    "        generated_images = generator(noise, training = True)\r\n",
    "\r\n",
    "        real_output = discriminator(images, training = True)\r\n",
    "        fake_output = discriminator(generated_images, training = True)\r\n",
    "\r\n",
    "        gen_loss = generator_loss(fake_output)\r\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\r\n",
    "\r\n",
    "    gen_mean_loss(gen_loss)\r\n",
    "    disc_mean_loss(disc_loss)\r\n",
    "\r\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n",
    "\r\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:39.372093Z",
     "iopub.status.busy": "2021-07-20T18:10:39.371265Z",
     "iopub.status.idle": "2021-07-20T18:10:39.373804Z",
     "shell.execute_reply": "2021-07-20T18:10:39.373382Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.356796Z"
    },
    "papermill": {
     "duration": 0.076228,
     "end_time": "2021-07-20T18:10:39.373914",
     "exception": false,
     "start_time": "2021-07-20T18:10:39.297686",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training function\r\n",
    "1. At start of each epoch, reset the `Mean` loss metrics\r\n",
    "2. For each batch in the epoch, call the `train_step` function\r\n",
    "3. Generate images using the `generator` on the seed noise data\r\n",
    "4. Every 5 epochs, save a checkpoint\r\n",
    "5. Display epoch metrics and progress"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_losses = []\r\n",
    "disc_losses = []\r\n",
    "\r\n",
    "def train(dataset, epochs):\r\n",
    "    for epoch in range(epochs):\r\n",
    "        start = time.time()\r\n",
    "\r\n",
    "        gen_mean_loss.reset_states()\r\n",
    "        disc_mean_loss.reset_states()\r\n",
    "        \r\n",
    "        print(f\"\\nTraining Epoch {epoch + EPOCH_START}\\n\")\r\n",
    "        \r\n",
    "        for batch_ind, image_batch in enumerate(dataset):\r\n",
    "            train_step(image_batch)\r\n",
    "\r\n",
    "            if (batch_ind + 1) % 10 == 0:\r\n",
    "                print(\". \", end = '')\r\n",
    "            if (batch_ind + 1) % 250 == 0:\r\n",
    "                print(f\"{batch_ind + 1}\")\r\n",
    "        \r\n",
    "        checkpoint.step.assign_add(1)\r\n",
    "\r\n",
    "        display.clear_output(wait = True)\r\n",
    "        \r\n",
    "        generate_images(seed_noise, True, epoch + EPOCH_START)\r\n",
    "\r\n",
    "        if (epoch + EPOCH_START) % 5 == 0:\r\n",
    "            ckpt_manager.save()\r\n",
    "            \r\n",
    "        gen_losses.append(gen_mean_loss.result())\r\n",
    "        disc_losses.append(disc_mean_loss.result())\r\n",
    "\r\n",
    "        print(f\"\\nEpoch: {epoch + EPOCH_START}\\n\")\r\n",
    "        print(f'Generator Loss: {gen_mean_loss.result():.4f}')\r\n",
    "        print(f'Discriminator Loss: {disc_mean_loss.result():.4f}')\r\n",
    "        print (f'Time elapsed: {time.time() - start:.2f} s')\r\n",
    "\r\n",
    "    display.clear_output(wait = True)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:39.513469Z",
     "iopub.status.busy": "2021-07-20T18:10:39.512669Z",
     "iopub.status.idle": "2021-07-20T18:10:39.515496Z",
     "shell.execute_reply": "2021-07-20T18:10:39.515034Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.367383Z"
    },
    "papermill": {
     "duration": 0.076273,
     "end_time": "2021-07-20T18:10:39.515609",
     "exception": false,
     "start_time": "2021-07-20T18:10:39.439336",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "train(train_ds, EPOCHS)\r\n",
    "\r\n",
    "print(f'Final Generator Loss: {gen_mean_loss.result()}')\r\n",
    "print(f'Final Discriminator Loss: {disc_mean_loss.result()}')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:10:39.655092Z",
     "iopub.status.busy": "2021-07-20T18:10:39.653941Z",
     "iopub.status.idle": "2021-07-20T18:54:51.462397Z",
     "shell.execute_reply": "2021-07-20T18:54:51.462828Z",
     "shell.execute_reply.started": "2021-07-20T17:01:41.379554Z"
    },
    "papermill": {
     "duration": 2651.881466,
     "end_time": "2021-07-20T18:54:51.463007",
     "exception": false,
     "start_time": "2021-07-20T18:10:39.581541",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output and Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display the images generated by the `generator` after training on the seed noise data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_images(seed_noise)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the loss and accuracy plots for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epoch_range = range(EPOCH_START, EPOCHS + EPOCH_START)\r\n",
    "plt.figure(figsize = (20, 8))\r\n",
    "\r\n",
    "plt.subplot(1, 2, 1)\r\n",
    "plt.plot(epoch_range, gen_losses)\r\n",
    "plt.xticks(epoch_range)\r\n",
    "plt.title('Generator loss', fontsize = 18)\r\n",
    "\r\n",
    "plt.subplot(1, 2, 2)\r\n",
    "plt.plot(epoch_range, disc_losses)\r\n",
    "plt.xticks(epoch_range)\r\n",
    "plt.title('Discriminator loss', fontsize = 18)\r\n",
    "\r\n",
    "plt.suptitle('Loss per epoch', fontsize = 24)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model saving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the trained models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generator.save(\"generator\")\r\n",
    "discriminator.save(\"discriminator\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2826.407805,
   "end_time": "2021-07-20T18:54:55.430701",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-20T18:07:49.022896",
   "version": "2.3.3"
  },
  "interpreter": {
   "hash": "dcbe076a40d8142e585077643c26fc4a9c0eed423ce3f041c8a5b2e5c8137bb1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}